\section{Solving systems of ordinary differential equations}
\label{sec:solvingsystems}

In physics, like other sciences, modeling a system often equates to solving
an initial value problem. An initial value problem can be described in terms
of a differential equation of the form

\begin{equation}
    \label{eq:ivpsystem}
    \dot{x}(t) = f\big(t,x(t)\big),\quad{}x(t_{0})=x_{0},
\end{equation}

where $x$ is an unknown function (scalar or vector) of time $t$. The function
$f$ is defined on an open subset $\Omega$ of $\mathbb{R}\times\mathbb{R}^{n}$.
The initial condition $(t_{0},x_{0})$ is a point in the domain of $f$, i.e.,
$(t_{0},x_{0})\in\Omega$. In higher dimensions (i.e., $n>1$) the differential
equation~\eqref{eq:ivpsystem} is replaced by a family of equations

\begin{equation}
\label{eq:ivpsystemhigherdimensions}
\begin{gathered}
    \dot{x}_{i}(t) = f_{i}\big(t,x_{1}(t),x_{2}(t),\ldots,x_{n}(t)\big),\quad
    x_{i}(t_{0})=x_{i,0},\quad{}i=1,\ldots,n \\
    \vct{x}(t) = \big(x_{1}(t),x_{2}(t),\ldots,x_{n}(t)\big)
\end{gathered}
\end{equation}

The system if nonlinear if the function $f$ in equation~\eqref{eq:ivpsystem},
alternatively, if at least one of the functions $f_{i}$ in equation
\eqref{eq:ivpsystemhigherdimensions}, is nonlinear in one or more of its
arguments.

\subsection{The Runge-Kutta family of numerical methods}
\label{sub:the_runge_kutta_family_of_numerical_methods}

For nonlinear systems, analytical solutions frequently do not exist. Thus, such
systems are often analyzed by means of numerical methods. In numerical analysis,
the Runge-Kutta family of methods are a frequently used collection of implicit
and explicit iterative methods, used in temporal discretization in order to
obtain numerical approximations of the \emph{true} solutions of systems like
\eqref{eq:ivpsystem}. The German mathematicians C. Runge and M. W. Kutta
developed the first of the family's methods at the turn of the twentieth century
\parencite[p.134 in the 2008 printing]{hairer1993solving}. The general scheme of
what is now known as a Runge-Kutta method is as follows: \\

\begin{defn}
    \label{def:generalrungekutta}
    Let $s$ be an integer and $a_{1,1},a_{1,2},\ldots,a_{1,s},a_{2,1},
    a_{2,2},\ldots,a_{2,s},\ldots,a_{s,1},a_{s,2},\ldots,a_{s,s}$,
    $b_{1},b_{2},\ldots,b_{s}$ and $c_{1},c_{2},\ldots,c_{s}$ be real
    coefficients. Let $h$ be the numerical step length used in the
    temporal discretization. Then, the method
\begin{equation}
    \label{eq:generalrungekutta}
    \begin{aligned}
        k_{i} &= f\bigg(t_{n}+c_{i}h,x_{n}+
                h\sum\limits_{j=1}^{s}a_{i,j}k_{j}\bigg),\quad{}i=1,\ldots,s\\
        x_{n+1} &= x_{n} + h\sum\limits_{i=1}^{s}b_{i}k_{i}
    \end{aligned}
\end{equation}

is called an \emph{s-stage Runge-Kutta method} for the system
\eqref{eq:ivpsystem}.
\end{defn}

The main reason to include multiple stages $s$ in a Runge-Kutta method,
is to improve the numerical accuracy of the computed solutions.
\textcite[p.2 in the 2010 printing]{hairer1993solving} define the \emph{order}
of a Runge-Kutta method as follows:\\

\begin{defn}
    \label{def:rungekuttaorder}
    A Runge-Kutta method \eqref{eq:generalrungekutta} is said to be of
    \emph{order} $p$ if, for sufficiently smooth systems \eqref{eq:ivpsystem},

    \begin{equation}
        \label{eq:rungekuttaorder}
        \norm{x_{n+1}-x(t_{n+1})} \leq Kh^{p+1},
    \end{equation}

    where $K$ is a numerical constant, i.e., if the Taylor series for the exact
    solution $x(t_{n+1})$ and the numerical solution $x_{n+1}$ coincide up to
    (and including) the term $h^p$.
\end{defn}

It is easy to show that if the local error of a Runge-Kutta method is of order
$p+1$, the global error, i.e., the total accumulated error resulting of
applying the algorithm a number of times, is expected to scale as $h^{p}$.
Showing this is left as an exercise for the interested reader.

In definition~\ref{def:generalrungekutta}, the matrix $(a_{i,j})$ is commonly
called the \emph{Runge-Kutta matrix}, while $b_{i}$ and $c_{i}$ are known as
the \emph{weights} and \emph{nodes}, respectively.  Since the 1960s, it has
been customary to represent Runge-Kutta methods~\eqref{eq:generalrungekutta}
symbolically, by means of mnemonic devices known as Butcher tableaus
\parencite[p.134 in the 2008 printing]{hairer1993solving}. The Butcher tableau
for a general \emph{s}-stage Runge-Kutta method, introduced in definition
\ref{def:generalrungekutta}, is presented in table~\ref{tab:generalbutcher}.

\begin{table}[htpb]
    \centering
    \caption[Butcher tableau representation of a general $s$-stage
                Runge-Kutta method]{Butcher tableau representation of a general
                    $s$-stage Runge-Kutta method.}
    \label{tab:generalbutcher}
    \[\renewcommand{\arraystretch}{1.25}
        \begin{array}{c|cccc}
            \toprule
            c_{1} & a_{1,1} & a_{1,2} & \ldots & a_{1,s}\\
            c_{2} & a_{2,1} & a_{2,2} & \ldots & a_{2,s}\\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            c_{s} & a_{s,1} & a_{s,2} & \ldots & a_{s,s}\\
            \hline
            & b_{1} & b_{2} & \ldots & b_{s}\\
            \bottomrule
    \end{array}
\]
\end{table}

For explicit Runge-Kutta methods, the Runge-Kutta matrix $(a_{i,j})$ is lower
triangular. Similarly, for fully implicit Runge-Kutta methods, the Runge-Kutta
matrix is upper triangular. Unlike explicit methods, implicit methods require
the solution of a linear system at every time level, making them more
computationally demanding than their explicit siblings. The main selling point
of implicit methods is that they are more numerically stable than explicit
methods. This property means that implicit methods are particularly well-suited
for \emph{stiff} systems, i.e., physical systems with highly disparate time
scales~\parencite[p.2 in the 2010 printing]{hairer1996solving}. For such systems,
most explicit methods are highly numerically unstable, unless the numerical step
size is made exceptionally small, rendering most explicit methods practically
useless. For \emph{nonstiff} systems, however, implicit methods behave similarly
to their explicit analogues in terms of numerical accuracy and
convergence properties.

During the first half of the twentieth century, a substantial amount of research
was conducted in order to develop numerically robust, high-order, explicit
Runge-Kutta methods. The idea was that using such methods would mean one could
resort to larger time increments $h$ without sacrificing precision in the
computational solution. However, the number of stages $s$ grows quicker than
linearly as a function of the required order $p$. It has been proven
that, for $p\geq5$, no explicit Runge-Kutta method of order $p$ with $s=p$
stages exists \parencite[p.173 in the 2008 printing]{hairer1993solving}. This is
one of the reasons for the attention shift from the latter half of the 1950s
and onwards, towards so-called \emph{embedded} Runge-Kutta methods.

The basic idea of embedded Runge-Kutta methods is that they, aside from the
numerical approximation $x_{n+1}$, yield a second approximation
$\widehat{x}_{n+1}$. The difference between the two approximations then yields
an estimate of the local error of the less precise result, which can be used for
automatic step size control. The trick is to construct two independent, explicit
Runge-Kutta methods which both use the \emph{same} function evaluations. This
results in practically obtaining the two solutions for the price of one, in
terms of computational complexity. The Butcher tableau of an embedded, general,
explicit Runge-Kutta method is illustrated in
\cref{tab:generalembeddedbutcher}.

\begin{table}[htpb]
    \centering
    \caption[Butcher tableau representation of general, embedded, explicit
    Runge-Kutta methods]{Butcher tableau representation of general, embedded,
        explicit Runge-Kutta methods.}
    \label{tab:generalembeddedbutcher}
    \[\renewcommand{\arraystretch}{1.25}
    \begin{array}{c|ccccc}
    \toprule
    0 \\
    c_{2} & a_{2,1} \\
    c_{3} & a_{3,1} & a_{3,2} \\
    \vdots & \vdots & \vdots & \ddots\\
    c_{s} & a_{s,1} & a_{s,2} & \ldots & a_{s,s-1}\\
    \hline
    & b_{1} & b_{2} & \ldots & b_{s-1} & b_{s} \\
    \hline
    & \widehat{b}_{1} & \widehat{b}_{2} & \ldots & \widehat{b}_{s-1}& \widehat{b}_{s}\\
    \bottomrule
    \end{array}
\]
\end{table}

For embedded methods, the coefficients are tuned such that

\begin{subequations}
    \begin{equation}
        \label{eq:embeddedsol}
        x_{n+1} = x_{n} + h\sum\limits_{i=1}^{s}b_{i}k_{i}
    \end{equation}
is of order $p$, and
    \begin{equation}
        \label{eq:embeddedinterp}
        \widehat{x}_{n+1} = x_{n} + h\sum\limits_{i=1}^{s}\widehat{b}_{i}k_{i}
    \end{equation}
\end{subequations}
is of order $\widehat{p}$, typically with $\widehat{p} = p \pm 1$.

In order to implement automatic step size control, the procedure suggested
by \textcite[pp.167--168 in the 2008 printing]{hairer1993solving} was followed
closely. A starting step size of $h=0.1$ was used throughout. For the first
solution step, the embedded integration method yields the two approximations
$x_{1}$ and $\widehat{x}_{1}$, yielding the difference $x_{1}-\widehat{x}_{1}$
as an estimate of the error of the less precise result. The idea is to enforce
the error of the numerical solution to satisfy componentwise:

\begin{equation}
    \label{eq:embeddederror}
    \abs{x_{1,i}-\widehat{x}_{1,i}} \leq \scem_{i},\quad{}%
    \scem_{i}=\atol_{i}+\max\big(\abs{x_{0,i}},\abs{x_{1,i}}\big)\cdot{}\rtol_{i}
\end{equation}

where $\atol_{i}$ and $\rtol_{i}$ are the desired numerical tolerances, prescribed
by the user. For this project, $\atol_{i}$ was always set equal to $\rtol_{i}$.

As a measure of the numerical error,

\begin{equation}
    \label{eq:embeddednumericalerror}
    \errem = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}%
    \Big(\frac{x_{1,i}-\widehat{x}_{1,i}}{\scem_{i}}\Big)^{2}}
\end{equation}

is used. Then, $\errem$ is compared to $1$ in order to find an optimal step
size. From~\cref{def:rungekuttaorder}, it follows that $\errem\approx{}Kh^{q+1}$,
where $q=\min(p,\widehat{p}\,)$. With $1\approx{}Kh_{\mathrm{opt}}^{q+1}$,
one finds the optimal step size

\begin{equation}
    \label{eq:embeddedoptimalstepsize}
    h_{\mathrm{opt}}=h\cdot\Big(\frac{1}{\errem}\Big)^{\frac{1}{q+1}}.
\end{equation}

If $\errem\leq1$, the solution step is accepted, the time level is increased by
$h$, and the step length is increased. Which of the two approximations $x_{n+1}$
or $\widehat{x}_{n+1}$ are used to continue the integration varies depending on the embedded method in
question. Continuing the integration with the higher order result is commonly
referred to as \emph{local extrapolation}. If $\errem>1$, the solution step is
rejected, the time level remains unaltered, and the step length is decreased.
The  procedure for updating the step length can be summarized as follows:


\begin{equation}
    \label{eq:embeddedstepsizeadjustment}
h_{\mathrm{new}}=\begin{cases}%
    \min(\facem_{\mathrm{max}}\cdot{}h,\facem\cdot{}h_{\mathrm{opt}}),&
\textnormal{if the solution step is accepted}\\
\facem\cdot{}h_{\mathrm{opt}},&\textnormal{if the solution step is rejected}%
\end{cases}
\end{equation}

where $\facem$ and $\facem_{\mathrm{max}}$ are numerical safety factors,
intended to prevent increasing the step size \emph{too} much. For this project,
$\facem=0.8$ and $\facem_{\mathrm{max}}=2.0$ were used throughout.

\subsection{The Runge-Kutta methods under consideration}
\label{sub:the_runge_kutta_methods_under_consideration}

Naturally, an abundance of Runge-Kutta methods exist. Many of them are
fine-tuned for specific constraints, such as problems of varying degrees of
stiffness. It is neither possible nor meaningful to investigate them all
in the context of general flow dynamics. For this reason, I consider two classes
of explicit Runge-Kutta methods, namely singlestep and adaptive stepsize
methods. From both classes, I include four different general-purpose ODE solvers
of varying order.

\subsubsection{Singlestep methods}
\label{ssub:singlestep_methods}

The singlestep methods under consideration are the classical, explicit
Runge-Kutta methods of orders one through to four, i.e., the \emph{Euler},
\emph{Heun}, \emph{Kutta} and \emph{classical Runge-Kutta} methods. The
Euler method is \nth{1} order accurate, and requires a single function
evaluation of the right hand side of the ordinary differential equation
\eqref{eq:ivpsystem} or~\eqref{eq:ivpsystemhigherdimensions} at each time step.
Its Butcher tableau representation can be found in~\cref{tab:butchereuler}.
It is the simplest explicit method for numerical integration of ordinary
differential equations. The Euler method is often used as a basis to construct
more complex methods, such as the Heun method, which is also known as the
\emph{improved Euler method} or the \emph{explicit trapezoidal rule}. The Heun
method is \nth{2} order accurate, and requires two function evaluations at each
time step. Its Butcher tableau representation can be found in
\cref{tab:butcherrk2}.

\input{mainmatter/theory/butchertableaus/euler.tex}

\input{mainmatter/theory/butchertableaus/rk2.tex}

The Kutta method is \nth{3} order accurate, and requires three function
evaluations of the right hand side of the ordinary differential equation
\eqref{eq:ivpsystem} or ~\eqref{eq:ivpsystemhigherdimensions} at each time
step. Its Butcher tableau representation can be found in \cref{tab:butcherrk3}.
The classical Runge-Kutta method is \nth{4} order accurate, and perhaps the most
well-known and frequently used of the four singlestep schemes discussed in this
project. One reason for its popularity is that it is exceptionally stable
numerically (i.e., of the aforementioned singlestep methods, the classical
Runge-Kutta method has the largest numerical stability domain). Another is that,
as mentioned previously, for $p\geq5$, no explicit Runge-Kutta method of order
$p$ with $s=p$ stages exist
\parencite[p.173 in the 2008 printing]{hairer1993solving} -- which means that
the required number of function evaluations grows at a disproportional rate with
the required accuracy order. For systems with right hand sides which are
computationally costly to evaluate, this means that one frequently is able to
obtain the desired numerical accuracy more effectively e.g.\ by using the
classical Runge-Kutta method with a finer step length. The Butcher tableau
representation of the classical Runge-Kutta method can be found in
\cref{tab:butcherrk4}.

\input{mainmatter/theory/butchertableaus/rk3.tex}

\input{mainmatter/theory/butchertableaus/rk4.tex}

\subsubsection{Adaptive stepsize methods}
\label{ssub:adaptive_stepsize_methods}

The adaptive stepsize methods under consideration are the Bogacki-Shampine
3(2) and 5(4) methods, and the Dormand-Prince 5(4) and 8(7) methods. The digit
outside of the parentheses indicates the order of the solution which is used
to continue the integration, while the digit within the parentheses indicates
the order of the interpolant solution. Note that the concept of \emph{order}
does not translate directly from singlestep methods, as a direct consequence
of the adaptive time step. Generally, lower order methods are more
suitable than higher order methods for cases where crude approximations of the
solution are sufficient. Bogacki and Shampine argue that their methods
outperform other methods of the same order
\parencite{bogacki1989pair,bogacki1996efficient}, a notion which, for the 5(4)
method, is supported by
\textcite[p.194 in the 2008 printing]{hairer1993solving}.

Butcher tableau representations of the aforementioned adaptive stepsize methods
can be found in
\cref{tab:butcherbs32,tab:butcherbs54,tab:butcherdopri54,tab:butcherdopri87},
the latter of which has been typeset in landscape orientation for the reader's
convenience. Three of the methods, namely the Bogacki-Shampine 3(2) and 5(4)
methods, in addition to the Dormand-Prince 5(4) method, possess the so-called
\emph{First Same As Last} property. This means that the last function evaluation
of an accepted step is exactly the same as the first function evaluation of the
next step. This is readily apparent from their Butcher tableaus, where the
$b$ coefficients correspond exactly with the last row the Runge-Kutta matrix.
The \emph{First Same As Last} property reduces the computational
cost of a successive step.

\input{mainmatter/theory/butchertableaus/bs32.tex}

\vspace{\fill}

\input{mainmatter/theory/butchertableaus/bs54.tex}

\input{mainmatter/theory/butchertableaus/dp54.tex}

\input{mainmatter/theory/butchertableaus/dp87.tex}


